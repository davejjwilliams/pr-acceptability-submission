{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef25c91",
   "metadata": {},
   "source": [
    "# Data Preparation Notebook (Annotating + Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58458b8f",
   "metadata": {},
   "source": [
    "## Imports and Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa6437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pr_df): 33596\n",
      "len(repo_df): 2807\n",
      "len(user_df): 1796\n",
      "\n",
      "len(pr_comments_df): 39122\n",
      "len(pr_reviews_df): 28875\n",
      "len(pr_review_comments_df): 26868\n",
      "\n",
      "len(pr_commits_df): 88576\n",
      "len(pr_commit_details_df): 711923\n",
      "\n",
      "len(related_issue_df): 4923\n",
      "len(issue_df): 4614\n",
      "\n",
      "len(pr_timeline_df): 325500\n",
      "\n",
      "len(pr_task_type_df): 33596\n",
      "\n",
      "len(human_pr_df): 6618\n",
      "len(human_pr_task_type_df): 6618\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Basic\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "user_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/user.parquet\")\n",
    "\n",
    "# Comments and reviews\n",
    "pr_comments_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_comments.parquet\")\n",
    "pr_reviews_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_reviews.parquet\")\n",
    "pr_review_comments_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_review_comments_v2.parquet\")\n",
    "\n",
    "# Commits\n",
    "pr_commits_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_commits.parquet\")\n",
    "pr_commit_details_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Related issues\n",
    "related_issue_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/related_issue.parquet\")\n",
    "issue_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/issue.parquet\")\n",
    "\n",
    "# Events\n",
    "pr_timeline_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_timeline.parquet\")\n",
    "\n",
    "# Task type\n",
    "pr_task_type_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "\n",
    "# Human-PR\n",
    "human_pr_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\n",
    "    \"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "\n",
    "print(f\"len(pr_df): {len(pr_df)}\")\n",
    "print(f\"len(repo_df): {len(repo_df)}\")\n",
    "print(f\"len(user_df): {len(user_df)}\")\n",
    "\n",
    "print(f\"\\nlen(pr_comments_df): {len(pr_comments_df)}\")\n",
    "print(f\"len(pr_reviews_df): {len(pr_reviews_df)}\")\n",
    "print(f\"len(pr_review_comments_df): {len(pr_review_comments_df)}\")\n",
    "\n",
    "print(f\"\\nlen(pr_commits_df): {len(pr_commits_df)}\")\n",
    "print(f\"len(pr_commit_details_df): {len(pr_commit_details_df)}\")\n",
    "\n",
    "\n",
    "print(f\"\\nlen(related_issue_df): {len(related_issue_df)}\")\n",
    "print(f\"len(issue_df): {len(issue_df)}\")\n",
    "\n",
    "print(f\"\\nlen(pr_timeline_df): {len(pr_timeline_df)}\")\n",
    "\n",
    "print(f\"\\nlen(pr_task_type_df): {len(pr_task_type_df)}\")\n",
    "\n",
    "print(f\"\\nlen(human_pr_df): {len(human_pr_df)}\")\n",
    "print(f\"len(human_pr_task_type_df): {len(human_pr_task_type_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25439b",
   "metadata": {},
   "source": [
    "## Annotating Features\n",
    "\n",
    "New columns added to both human and agentic PR dataframes:\n",
    "\n",
    "- accepted (bool) - merged PRs\n",
    "- rejected (bool) - PRs that were closed but not merged\n",
    "- turnaround_time (seconds) - number of seconds between creation and decision (merged/closed)\n",
    "- days_to_close (days) - number of days between creation and decision (merged/closed)\n",
    "\n",
    "New columns added to only agentic PR dataframes:\n",
    "\n",
    "- related_issue (bool) - if the PR has a related issue\n",
    "- has_open_related_issue (bool) - if at least one of the related issues are open\n",
    "- num_commits (int) - number of commits in the PR\n",
    "- num_files_changed (int) - number of files touched in the PR\n",
    "- touches_test_file (bool) - whether \"test\" appears in any of the filenames modified by the PR\n",
    "- lines_added (int) - number of lines added throughout all PR commits\n",
    "- lines_deleted (int) - number of lines deleted throughout all PR commits\n",
    "- total_churn (int) - total churn across all PR commits (lines added + lines deleted)\n",
    "- net_churn (int) - net churn across all PR commits (lines added - lines deleted)\n",
    "- num_bot_users (int) - how many bots were involved in the PR (either in comments or reviews)\n",
    "- num_human_users (int) - how many humans were involved in the PR (either in comments or reviews)\n",
    "- num_total_users (int) - how many users (bots & humans) were involved in the PR (either in comments or reviews)\n",
    "- num_comments (int) - total number of comments (bot & human)\n",
    "- num_human_comments (int) - number of human comments\n",
    "- num_bot_comments (int) - number of bot comments\n",
    "- num_reviews (int) - total number of reviews (bot & human)\n",
    "- num_human_reviews (int) - number of human reviews\n",
    "- num_bot_reviews (int) - number of bot reviews\n",
    "- has_comment (bool) - if the PR has at least one comment\n",
    "- has_review (bool) - if the PR has at least one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9b0057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gw/jg8766954y92j4x7kpzdhg680000gp/T/ipykernel_61964/3925933268.py:65: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['touches_test_file'] = df['touches_test_file'].fillna(False)\n",
      "/var/folders/gw/jg8766954y92j4x7kpzdhg680000gp/T/ipykernel_61964/3925933268.py:80: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  comment_users = pr_comments_df.groupby('pr_id').apply(\n",
      "/var/folders/gw/jg8766954y92j4x7kpzdhg680000gp/T/ipykernel_61964/3925933268.py:86: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  comment_counts = pr_comments_df.groupby('pr_id').apply(\n",
      "/var/folders/gw/jg8766954y92j4x7kpzdhg680000gp/T/ipykernel_61964/3925933268.py:95: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  review_users = pr_reviews_df.groupby('pr_id').apply(\n",
      "/var/folders/gw/jg8766954y92j4x7kpzdhg680000gp/T/ipykernel_61964/3925933268.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  review_counts = pr_reviews_df.groupby('pr_id').apply(\n"
     ]
    }
   ],
   "source": [
    "def annotate_pr_features(pr_dataframe, human=False):\n",
    "    \"\"\"\n",
    "    Annotate PR dataframe with additional features.\n",
    "\n",
    "    Parameters:\n",
    "    - pr_dataframe: DataFrame containing pull request data\n",
    "\n",
    "    Returns:\n",
    "    - Annotated PR DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = pr_dataframe.copy()\n",
    "\n",
    "    # Drop columns if they already exist\n",
    "    columns_to_drop = ['accepted', 'rejected', 'pending', 'turnaround_time', 'days_to_close', 'related_issue', 'has_open_related_issue', 'num_commits', 'num_files_changed', 'touches_test_file', 'lines_added', 'lines_deleted',\n",
    "                       'total_churn', 'net_churn', 'num_bot_users', 'num_human_users', 'num_total_users', 'num_comments', 'num_human_comments', 'num_bot_comments', 'num_reviews', 'num_human_reviews', 'num_bot_reviews', 'has_comment', 'has_review']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    df['accepted'] = df['merged_at'].notnull()\n",
    "    df['rejected'] = (df['state'] == 'closed') & df['merged_at'].isnull()\n",
    "    df['turnaround_time'] = (pd.to_datetime(\n",
    "        df['closed_at']) - pd.to_datetime(df['created_at'])).dt.total_seconds()\n",
    "    df['days_to_close'] = (pd.to_datetime(\n",
    "        df['closed_at']) - pd.to_datetime(df['created_at'])).dt.days + 1\n",
    "\n",
    "    if not human:\n",
    "        # Related Issue\n",
    "        df['related_issue'] = df['id'].isin(\n",
    "            related_issue_df['pr_id'].unique())\n",
    "\n",
    "        # Has Open Related Issue\n",
    "        merged_related_issues = related_issue_df.merge(\n",
    "            issue_df[['id', 'state']], left_on='issue_id', right_on='id', how='left')\n",
    "        pr_open_issues = merged_related_issues[merged_related_issues['state'] == 'open']\n",
    "        pr_has_open_issue = pr_open_issues['pr_id'].unique()\n",
    "        df['has_open_related_issue'] = df['id'].isin(pr_has_open_issue)\n",
    "\n",
    "        # Number of commits per PR\n",
    "        pr_commit_counts = pr_commits_df.groupby(\n",
    "            'pr_id').size().reset_index(name='num_commits')\n",
    "        df = df.merge(pr_commit_counts, left_on='id',\n",
    "                      right_on='pr_id', how='left')\n",
    "        df['num_commits'] = df['num_commits'].fillna(0).astype(int)\n",
    "        df = df.drop('pr_id', axis=1)\n",
    "\n",
    "        # Number of files changed per PR\n",
    "        pr_files_changed = pr_commit_details_df.groupby(\n",
    "            'pr_id')['filename'].nunique().reset_index()\n",
    "        pr_files_changed.columns = ['id', 'num_files_changed']\n",
    "        df = df.merge(pr_files_changed, on='id', how='left')\n",
    "\n",
    "        def touches_test_file(filenames):\n",
    "            for filename in filenames:\n",
    "                if filename is not None and 'test' in filename.lower():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        pr_files = pr_commit_details_df.groupby(\n",
    "            'pr_id')['filename'].apply(list).reset_index()\n",
    "        pr_files['touches_test_file'] = pr_files['filename'].apply(\n",
    "            touches_test_file)\n",
    "        df = df.merge(pr_files[['pr_id', 'touches_test_file']],\n",
    "                      left_on='id', right_on='pr_id', how='left')\n",
    "        df = df.drop('pr_id', axis=1)\n",
    "        df['touches_test_file'] = df['touches_test_file'].fillna(False)\n",
    "\n",
    "        # PR Size (number of lines added and deleted across all commits)\n",
    "        pr_size = pr_commit_details_df.groupby('pr_id').agg(\n",
    "            {'additions': 'sum', 'deletions': 'sum'}).reset_index()\n",
    "        pr_size.columns = ['id', 'lines_added', 'lines_deleted']\n",
    "        df = df.merge(pr_size, on='id', how='left')\n",
    "\n",
    "        # Net Churn & Total Churn\n",
    "        df['net_churn'] = df['lines_added'] - df['lines_deleted']\n",
    "        df['total_churn'] = df['lines_added'] + df['lines_deleted']\n",
    "\n",
    "        ### Number of Human/Bot/Total Reviews/Comments/Involvement ###\n",
    "\n",
    "        # Get unique users from comments\n",
    "        comment_users = pr_comments_df.groupby('pr_id').apply(\n",
    "            lambda x: list(zip(x['user'], x['user_type']))\n",
    "        ).reset_index()\n",
    "        comment_users.columns = ['pr_id', 'comment_users']\n",
    "\n",
    "        # Get comment counts by type\n",
    "        comment_counts = pr_comments_df.groupby('pr_id').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'num_comments': len(x),\n",
    "                'num_human_comments': sum(x['user_type'] == 'User'),\n",
    "                'num_bot_comments': sum(x['user_type'] == 'Bot')\n",
    "            })\n",
    "        ).reset_index()\n",
    "\n",
    "        # Get unique users from reviews\n",
    "        review_users = pr_reviews_df.groupby('pr_id').apply(\n",
    "            lambda x: list(zip(x['user'], x['user_type']))\n",
    "        ).reset_index()\n",
    "        review_users.columns = ['pr_id', 'review_users']\n",
    "\n",
    "        # Get review counts by type\n",
    "        review_counts = pr_reviews_df.groupby('pr_id').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'num_reviews': len(x),\n",
    "                'num_human_reviews': sum(x['user_type'] == 'User'),\n",
    "                'num_bot_reviews': sum(x['user_type'] == 'Bot')\n",
    "            })\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge and combine users\n",
    "        user_interactions = comment_users.merge(\n",
    "            review_users, on='pr_id', how='outer')\n",
    "        user_interactions['comment_users'] = user_interactions['comment_users'].apply(\n",
    "            lambda x: x if isinstance(x, list) else [])\n",
    "        user_interactions['review_users'] = user_interactions['review_users'].apply(\n",
    "            lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "        # Combine all users and count unique bots and humans\n",
    "        def count_user_types(row):\n",
    "            all_users = row['comment_users'] + row['review_users']\n",
    "            unique_users = list(set(all_users))\n",
    "\n",
    "            num_bots = sum(\n",
    "                1 for user, user_type in unique_users if user_type == 'Bot')\n",
    "            num_humans = sum(\n",
    "                1 for user, user_type in unique_users if user_type == 'User')\n",
    "            num_total = len(unique_users)\n",
    "\n",
    "            return pd.Series({\n",
    "                'num_bot_users': num_bots,\n",
    "                'num_human_users': num_humans,\n",
    "                'num_total_users': num_total\n",
    "            })\n",
    "\n",
    "        user_counts = user_interactions.apply(count_user_types, axis=1)\n",
    "        user_interactions = pd.concat(\n",
    "            [user_interactions[['pr_id']], user_counts], axis=1)\n",
    "\n",
    "        # Merge all counts together\n",
    "        user_interactions = user_interactions.merge(\n",
    "            comment_counts, on='pr_id', how='outer')\n",
    "        user_interactions = user_interactions.merge(\n",
    "            review_counts, on='pr_id', how='outer')\n",
    "\n",
    "        # Merge with PR dataframe\n",
    "        df = df.merge(user_interactions, left_on='id',\n",
    "                      right_on='pr_id', how='left')\n",
    "        df = df.drop('pr_id', axis=1)\n",
    "\n",
    "        # Fill NaN values with 0\n",
    "        for col in ['num_bot_users', 'num_human_users', 'num_total_users',\n",
    "                    'num_comments', 'num_human_comments', 'num_bot_comments',\n",
    "                    'num_reviews', 'num_human_reviews', 'num_bot_reviews']:\n",
    "            df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "        # Has Comment\n",
    "        df['has_comment'] = df['num_comments'] > 0\n",
    "\n",
    "        # Has Review\n",
    "        df['has_review'] = df['num_reviews'] > 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply annotation to both dataframes\n",
    "pr_df = annotate_pr_features(pr_df)\n",
    "human_pr_df = annotate_pr_features(human_pr_df, human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf5043",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "We apply the following filters to the dataset:\n",
    "\n",
    "- Restricting to repos with Apache-2.0 or MIT licences\n",
    "- Closed (merged/rejected) PRs only\n",
    "- At least one interaction that isn't by the author before PR decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filtering(pr_dataframe):\n",
    "    \"\"\"\n",
    "    Filter PR dataframe.\n",
    "\n",
    "    Filtering Steps:\n",
    "    1. Keep only PRs with non-author reviews or comments.\n",
    "    2. Keep only closed PRs.\n",
    "    3. Filter repositories with Apache-2.0 or MIT licences.\n",
    "    4. Filter top 10% for specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    - pr_dataframe: DataFrame containing pull request data\n",
    "\n",
    "    Returns:\n",
    "    - Filtered PR DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pr_dataframe.copy()\n",
    "\n",
    "    # Get PR IDs that exist in this dataframe\n",
    "    existing_pr_ids = df['id'].unique()\n",
    "\n",
    "    # PRs with non-author reviews (filter to only PRs in this dataframe)\n",
    "    relevant_reviews = pr_reviews_df[pr_reviews_df['pr_id'].isin(\n",
    "        existing_pr_ids)]\n",
    "    df_indexed = df.set_index('id')\n",
    "    \n",
    "    # Filter reviews by non-author AND created before PR closed_at\n",
    "    non_author_reviews_before_close = relevant_reviews[\n",
    "        (relevant_reviews['user'] != df_indexed.loc[relevant_reviews['pr_id'], 'user'].values) &\n",
    "        (pd.to_datetime(relevant_reviews['submitted_at']) < pd.to_datetime(df_indexed.loc[relevant_reviews['pr_id'], 'closed_at'].values))\n",
    "    ]['pr_id'].unique()\n",
    "\n",
    "    # PRs with non-author comments (filter to only PRs in this dataframe)\n",
    "    relevant_comments = pr_comments_df[pr_comments_df['pr_id'].isin(\n",
    "        existing_pr_ids)]\n",
    "    \n",
    "    # Filter comments by non-author AND created before PR closed_at\n",
    "    non_author_comments_before_close = relevant_comments[\n",
    "        (relevant_comments['user'] != df_indexed.loc[relevant_comments['pr_id'], 'user'].values) &\n",
    "        (pd.to_datetime(relevant_comments['created_at']) < pd.to_datetime(df_indexed.loc[relevant_comments['pr_id'], 'closed_at'].values))\n",
    "    ]['pr_id'].unique()\n",
    "\n",
    "    # Union of PRs with non-author reviews and comments\n",
    "    prs_with_non_author_interaction = set(\n",
    "        non_author_reviews_before_close) | set(non_author_comments_before_close)\n",
    "    \n",
    "    # Keep only PRs with non-author review/comment\n",
    "    df = df[df['id'].isin(prs_with_non_author_interaction)]\n",
    "    df = df[df['state'] == 'closed']  # Only closed PRs\n",
    "\n",
    "    # Filter repositories with Apache-2.0 or MIT licenses\n",
    "    apache_mit_repos = repo_df[repo_df['license'].isin(\n",
    "        ['Apache-2.0', 'MIT'])]['id'].unique()\n",
    "\n",
    "    # Keep only PRs from those repositories\n",
    "    df = df[df['repo_id'].isin(apache_mit_repos)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Filter dataframe\n",
    "pr_df = df_filtering(pr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffba86a",
   "metadata": {},
   "source": [
    "## Filter Other Tabled for Selected PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0ec4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered PR count: 7156\n",
      "Filtered pr_comments_df: 21243\n",
      "Filtered pr_reviews_df: 17746\n",
      "Filtered pr_review_comments_df: 15316\n",
      "Filtered pr_commits_df: 32679\n",
      "Filtered pr_commit_details_df: 235683\n",
      "Filtered pr_timeline_df: 124241\n",
      "Filtered pr_task_type_df: 7156\n",
      "Filtered related_issue_df: 2296\n",
      "Filtered human_pr_df: 6618\n",
      "Filtered human_pr_task_type_df: 6618\n",
      "Filtered repo_df: 1054\n",
      "Filtered user_df: 571\n",
      "Filtered issue_df: 2207\n"
     ]
    }
   ],
   "source": [
    "# Get the list of PR IDs from pr_df\n",
    "filtered_pr_ids = pr_df['id'].unique()\n",
    "\n",
    "# Filter all PR-related dataframes\n",
    "pr_comments_df = pr_comments_df[pr_comments_df['pr_id'].isin(filtered_pr_ids)]\n",
    "pr_reviews_df = pr_reviews_df[pr_reviews_df['pr_id'].isin(filtered_pr_ids)]\n",
    "# pr_review_comments_df = pr_review_comments_df[pr_review_comments_df['pull_request_url'].str.extract(r'/(\\d+)$')[0].astype(int).isin(filtered_pr_ids)]\n",
    "\n",
    "filtered_review_ids = pr_reviews_df[pr_reviews_df['pr_id'].isin(filtered_pr_ids)]['id'].unique()\n",
    "pr_review_comments_df = pr_review_comments_df[pr_review_comments_df['pull_request_review_id'].isin(filtered_review_ids)]\n",
    "\n",
    "\n",
    "pr_commits_df = pr_commits_df[pr_commits_df['pr_id'].isin(filtered_pr_ids)]\n",
    "pr_commit_details_df = pr_commit_details_df[pr_commit_details_df['pr_id'].isin(filtered_pr_ids)]\n",
    "pr_timeline_df = pr_timeline_df[pr_timeline_df['pr_id'].isin(filtered_pr_ids)]\n",
    "pr_task_type_df = pr_task_type_df[pr_task_type_df['id'].isin(filtered_pr_ids)]\n",
    "related_issue_df = related_issue_df[related_issue_df['pr_id'].isin(filtered_pr_ids)]\n",
    "\n",
    "# Get unique repo IDs and user IDs from filtered PRs\n",
    "filtered_repo_ids = pr_df['repo_id'].unique()\n",
    "filtered_user_ids = pr_df['user_id'].unique()\n",
    "\n",
    "# Filter repo and user dataframes\n",
    "repo_df = repo_df[repo_df['id'].isin(filtered_repo_ids)]\n",
    "user_df = user_df[user_df['id'].isin(filtered_user_ids)]\n",
    "\n",
    "# Get unique issue IDs from related_issue_df\n",
    "filtered_issue_ids = related_issue_df['issue_id'].unique()\n",
    "issue_df = issue_df[issue_df['id'].isin(filtered_issue_ids)]\n",
    "\n",
    "print(f\"Filtered PR count: {len(pr_df)}\")\n",
    "print(f\"Filtered pr_comments_df: {len(pr_comments_df)}\")\n",
    "print(f\"Filtered pr_reviews_df: {len(pr_reviews_df)}\")\n",
    "print(f\"Filtered pr_review_comments_df: {len(pr_review_comments_df)}\")\n",
    "print(f\"Filtered pr_commits_df: {len(pr_commits_df)}\")\n",
    "print(f\"Filtered pr_commit_details_df: {len(pr_commit_details_df)}\")\n",
    "print(f\"Filtered pr_timeline_df: {len(pr_timeline_df)}\")\n",
    "print(f\"Filtered pr_task_type_df: {len(pr_task_type_df)}\")\n",
    "print(f\"Filtered related_issue_df: {len(related_issue_df)}\")\n",
    "print(f\"Filtered human_pr_df: {len(human_pr_df)}\")\n",
    "print(f\"Filtered human_pr_task_type_df: {len(human_pr_task_type_df)}\")\n",
    "print(f\"Filtered repo_df: {len(repo_df)}\")\n",
    "print(f\"Filtered user_df: {len(user_df)}\")\n",
    "print(f\"Filtered issue_df: {len(issue_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e24579",
   "metadata": {},
   "source": [
    "## Save Filtered Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9541eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes_to_csv(subdirectory):\n",
    "    # Check if ./data/{subdirectory} directory exists\n",
    "    data_dir = \"./data\"\n",
    "    sub_dir = os.path.join(data_dir, subdirectory)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "\n",
    "    # Save all dataframes to CSV (will overwrite existing files)\n",
    "    pr_df.to_csv(os.path.join(sub_dir, \"pull_request.csv\"), index=False)\n",
    "    repo_df.to_csv(os.path.join(sub_dir, \"repository.csv\"), index=False)\n",
    "    user_df.to_csv(os.path.join(sub_dir, \"user.csv\"), index=False)\n",
    "    pr_comments_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_comments.csv\"), index=False)\n",
    "    pr_reviews_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_reviews.csv\"), index=False)\n",
    "    pr_review_comments_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_review_comments.csv\"), index=False)\n",
    "    pr_commits_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_commits.csv\"), index=False)\n",
    "    pr_commit_details_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_commit_details.csv\"), index=False)\n",
    "    related_issue_df.to_csv(os.path.join(\n",
    "        sub_dir, \"related_issue.csv\"), index=False)\n",
    "    issue_df.to_csv(os.path.join(sub_dir, \"issue.csv\"), index=False)\n",
    "    pr_timeline_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_timeline.csv\"), index=False)\n",
    "    pr_task_type_df.to_csv(os.path.join(\n",
    "        sub_dir, \"pr_task_type.csv\"), index=False)\n",
    "    human_pr_df.to_csv(os.path.join(\n",
    "        sub_dir, \"human_pull_request.csv\"), index=False)\n",
    "    human_pr_task_type_df.to_csv(os.path.join(\n",
    "        sub_dir, \"human_pr_task_type.csv\"), index=False)\n",
    "\n",
    "\n",
    "save_dataframes_to_csv(\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33707a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered PR count: 7156\n"
     ]
    }
   ],
   "source": [
    "test_pr_df = pd.read_csv('data/filtered/pull_request.csv')\n",
    "print(f\"Filtered PR count: {len(test_pr_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
